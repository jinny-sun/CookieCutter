{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CookieCutter\n",
    "The goal of this project is to predict the calories per serving of a recipe based on the ingredients list.\n",
    "\n",
    "Training data was scraped from AllRecipes.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multireplace(string, replacements, ignore_case=False):\n",
    "    \"\"\"\n",
    "    Given a string and a replacement map, it returns the replaced string.\n",
    "    :param str string: string to execute replacements on\n",
    "    :param dict replacements: replacement dictionary {value to find: value to replace}\n",
    "    :param bool ignore_case: whether the match should be case insensitive\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # If case insensitive, normalize the old string so that later a replacement\n",
    "    # can be found. For instance with {\"HEY\": \"lol\"} we should match and find a replacement for \"hey\",\n",
    "    # \"HEY\", \"hEy\", etc.\n",
    "    \n",
    "    if ignore_case:\n",
    "\n",
    "        def normalize_old(s):\n",
    "            return s.lower()\n",
    "\n",
    "        re_mode = re.IGNORECASE\n",
    "\n",
    "    else:\n",
    "\n",
    "        def normalize_old(s):\n",
    "            return s\n",
    "\n",
    "        re_mode = 0\n",
    "\n",
    "    replacements = {\n",
    "        normalize_old(key): val\n",
    "        for key, val in replacements.items()\n",
    "    }\n",
    "\n",
    "    # Place longer ones first to keep shorter substrings from matching where the longer ones should take place\n",
    "    # For instance given the replacements {'ab': 'AB', 'abc': 'ABC'} against the string 'hey abc', it should produce\n",
    "    # 'hey ABC' and not 'hey ABc'\n",
    "    rep_sorted = sorted(replacements, key=len, reverse=True)\n",
    "    rep_escaped = map(re.escape, rep_sorted)\n",
    "\n",
    "    # Create a big OR regex that matches any of the substrings to replace\n",
    "    pattern = re.compile(\"|\".join(rep_escaped), re_mode)\n",
    "\n",
    "    # For each match, look up the new string in the replacements, being the key the normalized old string\n",
    "    return pattern.sub(\n",
    "        lambda match: replacements[normalize_old(match.group(0))], string)\n",
    "\n",
    "\n",
    "def string_replace(orig_string):\n",
    "    \"\"\"\n",
    "    Replace whitespace characters with semicolon\n",
    "    \"\"\"\n",
    "    new_string = re.sub(' {2,}', ' ', orig_string).replace(\"\\n\", \";\").replace(\"; ;\", \";\")\n",
    "    return (new_string)\n",
    "\n",
    "\n",
    "def get_ingredients(orig_string):\n",
    "    \"\"\"\n",
    "    Separate numeric and text characters in a string\n",
    "    \"\"\"\n",
    "    ing_regex = ('(\\d+/*\\d*\\s*\\d*/*\\d*)\\s(\\w+\\s*.*?);')\n",
    "    all_ing = re.findall(ing_regex, orig_string)\n",
    "    return (all_ing)\n",
    "\n",
    "\n",
    "def get_quantity(regex_tuple):\n",
    "    \"\"\"\n",
    "    Separate tupule into two columns\n",
    "    \"\"\"\n",
    "    quantity = [y[0] for y in regex_tuple]\n",
    "    units_with_ingredient = [y[1] for y in regex_tuple]\n",
    "    df_of_units = pd.DataFrame({\n",
    "        'quantity': quantity,\n",
    "        'ingredient': units_with_ingredient\n",
    "    })\n",
    "    return (df_of_units)\n",
    "\n",
    "\n",
    "def match_uids(originaldf, longdf):\n",
    "    \"\"\"\n",
    "    Merge two dataframs using unique identifier\n",
    "    \"\"\"\n",
    "    for row in range(0, len(originaldf)):\n",
    "        longdf[row]['recipe_key'] = originaldf['recipe_key'][row]\n",
    "        longdf[row]['calPerServing'] = originaldf['calPerServing'][row]\n",
    "        longdf[row]['totalCal'] = originaldf['totalCal'][row]\n",
    "        longdf[row]['servings'] = originaldf['servings'][row]\n",
    "        longdf[row]['name'] = originaldf['name'][row]\n",
    "    return (longdf)\n",
    "\n",
    "\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove anything in parentheses\n",
    "    2. Lowercase all text\n",
    "    3. Remove all hypenated words\n",
    "    4. Remove all punctuation\n",
    "    5. Remove all whitespace\n",
    "    6. Remove numbers\n",
    "    7. Remove plurals\n",
    "    8. Remove all english stopwords & unwanted text\n",
    "    9. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    def lemmatize(string):\n",
    "        for word in re.findall(r\"[a-z]+\", string):\n",
    "            string = string.replace(\n",
    "                word,\n",
    "                wnl.lemmatize(word, 'n') if 's' in word[-3:] else word)\n",
    "        return string\n",
    "\n",
    "    unwanted_text = [\n",
    "        'dash', 'pinch', 'teaspoon', 'tablespoon', 'fluid', 'cup', 'pint',\n",
    "        'quart', 'ounce', 'oz', 'pound', 'rack', 'small', 'medium', 'large',\n",
    "        'crushed', 'grated', 'skinless', 'boneless', 'melted', 'fresh',\n",
    "        'diced', 'minced', 'thinly', 'dry', 'dried', 'halved', 'taste',\n",
    "        'frying', 'lean', 'drained', 'jars', 'grated', 'clove', 'slice',\n",
    "        'eaches', 'whole', 'cube', 'thick', 'unit', 'freshly', 'finely',\n",
    "        'splash', 'semisweet', 'chip', 'extract', 'spread', 'powder', 'room',\n",
    "        'temperature', 'brown', 'cooking', 'yolk', 'ground', 'package', 'mix',\n",
    "        'cake', 'plain', 'goody', 'light', 'wheat', 'piece', 'substitute',\n",
    "        'mini', 'kosher', 'crispy', 'minature', 'chunk', 'dark', 'bit',\n",
    "        'square', 'boiling', 'bag', 'crumb', 'popsicle', 'stick', 'zest',\n",
    "        'cereal', 'bar', 'tart', 'nib', 'tennessee', 'turbinado', 'baking',\n",
    "        'pack', 'spice', 'moist', 'miniarature', 'crunchy', 'morsel', 'nugget',\n",
    "        'candy', 'crisp', 'super', 'fine', 'decoration', 'sucralose', 'puree',\n",
    "        'pureed', 'rainbow', 'cut', 'frozen', 'broken', 'round', 'concentrate',\n",
    "        'miniature', 'cooky', 'virgin', 'dusting', 'half', 'baby', 'food',\n",
    "        'jar', 'seedless', 'container', 'box', 'granule', 'filling', 'cold',\n",
    "        'super', 'ripe', 'moisture', 'packet', 'instant', 'mint', 'ripe',\n",
    "        'sea', 'coarse', 'fun', 'size', 'funsize', 'bulk', 'chopped', 'torn',\n",
    "        'inch', 'shell', 'quality', 'strap', 'bittersweet', 'gallon', 'pure',\n",
    "        'cane', 'liquid', 'drop', 'hard', 'yellow', 'black', 'strap', 'kiss',\n",
    "        'protein', 'supplement', 'dessert', 'topping'\n",
    "    ]\n",
    "\n",
    "    # Remove anything in parenthesis\n",
    "    mess = re.sub(r\"\\([^\\)]+\\)\", '', mess)\n",
    "    # Make everything lowercase\n",
    "    mess = mess.lower()\n",
    "    # Remove non-word punctuation\n",
    "    mess = ' '.join(re.findall(\n",
    "        r\"[-,''\\w]+\", mess))  # This leaves some commas as a character #\n",
    "    mess = re.sub(r\"\\,\", ' ', mess)\n",
    "    # Remove hypenated words\n",
    "    mess = re.sub(r\"(?=\\S*['-])([a-zA-Z'-]+)\", '',\n",
    "                  mess)  # remove hypenated words\n",
    "    # Remove numbers\n",
    "    mess = ''.join([i for i in mess if not i.isdigit()])\n",
    "    # Remove plurals\n",
    "    mess = lemmatize(mess)\n",
    "    #clean excess whitespace\n",
    "    mess = re.sub(r\"\\s+\", ' ', mess).strip()\n",
    "    # Remove stopwords\n",
    "    mess = [\n",
    "        word for word in mess.split()\n",
    "        if word.lower() not in stopwords.words('english')\n",
    "    ]\n",
    "    mess = [word for word in mess if word.lower() not in unwanted_text]\n",
    "    mess = ' '.join(mess)\n",
    "    return (mess.split())\n",
    "\n",
    "\n",
    "def convert_fractions(quantity):\n",
    "    \"\"\"\n",
    "    Convert fractions into decimals\n",
    "    \"\"\"\n",
    "    from fractions import Fraction\n",
    "    return float(sum(Fraction(s) for s in quantity.split()))\n",
    "\n",
    "\n",
    "def pos_tagger(tokens, pos_tag):\n",
    "    \"\"\"\n",
    "    Select tokens that have noun part of speech tag\n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    return ([token[0] for token in tagged if token[1] in pos_tag])\n",
    "\n",
    "\n",
    "def word_quantity(ing_column, norm_quant_column, orig_dataframe):\n",
    "    \"\"\"\n",
    "    Repeat word based on quantity of ingredient.\n",
    "    \"\"\"\n",
    "    dummy_df = orig_dataframe.copy()\n",
    "    dummy_df['ingredient'] = dummy_df[ing_column].astype(str) + ' '\n",
    "    zipped = list(zip(dummy_df[ing_column], dummy_df[norm_quant_column]))\n",
    "    inglist = [t[0] * t[1] for t in zipped]\n",
    "    final_df = pd.DataFrame(inglist, columns=['ingredient'])\n",
    "    final_df[[\n",
    "        'recipe_key', 'totalCal', 'calPerServing', 'name', 'ingredient_key'\n",
    "    ]] = orig_dataframe[[\n",
    "        'recipe_key', 'totalCal', 'calPerServing', 'name', 'index'\n",
    "    ]]\n",
    "\n",
    "    # Create multiIndex / hierarchical Dataframe\n",
    "    tuples = list(zip(*[final_df['recipe_key'], final_df['ingredient_key']]))\n",
    "    index = pd.MultiIndex.from_tuples(tuples,\n",
    "                                      names=['recipe_key', 'ingredient_key'])\n",
    "    final_df.set_index(index, inplace=True)\n",
    "    final_df.rename(columns={'recipe_key': 'key'}, inplace=True)\n",
    "    #     return(final_df)\n",
    "\n",
    "    X_ing = final_df.groupby('recipe_key')['ingredient'].apply(\n",
    "        ' '.join)  # join list into one string per recipe\n",
    "    X_ing = pd.DataFrame(X_ing)\n",
    "    return (X_ing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean ingredient text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('cookie_recipes.csv') # all cookie recipes\n",
    "\n",
    "# Create unique id\n",
    "df['recipe_key'] = df['url'].apply(lambda x:int(re.findall(r\"\\d+\", x)[0]))\n",
    "\n",
    "# Calculate total calories per recipe\n",
    "df['totalCal'] = df['calPerServing']*df['servings']\n",
    "\n",
    "# Filter for recipes with 12-64 servings and < 10,000 total calories\n",
    "df = df[(df['servings']<=64) & \n",
    "        (df['servings']>=12) & \n",
    "        (df['totalCal']<10000)] \n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Clean ingredient text\n",
    "dict_unicode = {'\\u2009': '', '½':' 1/2', '⅓':'1/3', '⅔':'2/3', '¼':'1/4', '¾':'3/4', '⅕':'1/5', \n",
    "                '⅖':'2/5', '⅗':'3/5', '⅘':'4/5', '⅙':'1/6', '⅚':'5/6', '⅐':'1/7', '⅛':'1/8', \n",
    "                '⅜':'3/8', '⅝':'5/8', '⅞':'7/8', '⅑':'1/9', '⅒':'1/10'}\n",
    "df['ingredients'] = [item + ';' for item in df['ingredients']] # add semicolon at end of each string for easier regex filtering\n",
    "df['ingredients'] = [multireplace(x, dict_unicode) for x in df['ingredients']] # replace unicode characters\n",
    "df['ingredients'] = [string_replace(x) for x in df['ingredients']] # remove whitespace\n",
    "ing = [get_ingredients(x) for x in df['ingredients']] # separate ingredients into list of list of tupules of ingredient strings\n",
    "\n",
    "df_ing = [get_quantity(x) for x in ing] # separate units of measure and ingredients & creates a pandas dataframe for each recipe\n",
    "\n",
    "clean_df = match_uids(df, df_ing) # pull unique id, calorie (outcome variable), number of servings, and number of ingredients from original dataframe\n",
    "clean_df = pd.concat(clean_df) # concat list of pandas dataframes into one dataframe\n",
    "clean_df['quantity'] = [convert_fractions(x) for x in clean_df['quantity']] # convert fractions into integers\n",
    "clean_df = clean_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize quantity of ingredients to grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert measurements to normalized unit  (1 Unit= 1 grams)\n",
    "clean_df['unit'] = np.where(clean_df.ingredient.str.contains(\"dash\"), .3,\n",
    "            np.where(clean_df.ingredient.str.contains(\"pinch\"), .6,\n",
    "            np.where(clean_df.ingredient.str.contains(\"teaspoon\"), 5, \n",
    "            np.where(clean_df.ingredient.str.contains(\"tablespoon\"), 3,\n",
    "            np.where(clean_df.ingredient.str.contains(\"fluid\"), 30,\n",
    "            np.where(clean_df.ingredient.str.contains(\"cup\"), 240, \n",
    "            np.where(clean_df.ingredient.str.contains(\"pint\"), 473,\n",
    "            np.where(clean_df.ingredient.str.contains(\"quart\"), 980,\n",
    "            np.where(clean_df.ingredient.str.contains(\"ounce\"), 28,\n",
    "            np.where(clean_df.ingredient.str.contains(\"oz\"), 28, \n",
    "            np.where(clean_df.ingredient.str.contains(\"pound\"), 454,\n",
    "            np.where(clean_df.ingredient.str.contains(\"rack\"), 908,\n",
    "            np.where(clean_df.ingredient.str.contains(\"small\"), 50,\n",
    "            np.where(clean_df.ingredient.str.contains(\"medium\"), 60,\n",
    "            np.where(clean_df.ingredient.str.contains(\"large\"), 70,\n",
    "            3))))))))))))))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize ingredient text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenization = convert text string into list of tokens, or words, we want (i.e., cleaned version of words).\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "clean_df['ingredient']=[text_process(x) for x in clean_df['ingredient']]\n",
    "\n",
    "# Total quantity of each ingredient needed for recipe (grams* quantity) and condense into a list.\n",
    "clean_df['norm_quant'] = round(clean_df['unit']*clean_df['quantity'])\n",
    "clean_df['norm_quant'] = clean_df['norm_quant'].astype(int)\n",
    "\n",
    "# One word per ingredient - keep only nouns, join multiple words as one string\n",
    "clean_df['ingredient'] = [pos_tagger(tokens, ['NN']) for tokens in clean_df['ingredient']]\n",
    "clean_df['ingredient'] = [''.join(tokens) for tokens in clean_df['ingredient']]\n",
    "\n",
    "# Repeat word by normalized quantity\n",
    "X_ing = word_quantity('ingredient','norm_quant',clean_df)\n",
    "X_ing[['orig_ing', 'name', 'servings']] = df.set_index('recipe_key')[['ingredients', 'name', 'servings']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create feature and outcome dataframe\n",
    "y_cal = df.set_index('recipe_key')[['totalCal', 'calPerServing', 'name','servings']].sort_index().copy()\n",
    "X_keys = df.reset_index(drop=True)['recipe_key']\n",
    "\n",
    "# Train test split (80:20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "key_train, key_test, y_train, y_test = train_test_split(\n",
    "    X_keys, y_cal, test_size=0.2, random_state=101)\n",
    "\n",
    "# Separate feature and outcome dataframes based on key\n",
    "X_train = X_ing.loc[key_train]\n",
    "X_test = X_ing.loc[key_test]\n",
    "y_train = y_cal.loc[key_train]\n",
    "y_test = y_cal.loc[key_test]\n",
    "\n",
    "X_train.sort_index(inplace=True)\n",
    "X_test.sort_index(inplace=True)\n",
    "y_train.sort_index(inplace=True)\n",
    "y_test.sort_index(inplace=True)\n",
    "\n",
    "# Remove extreme edge cases\n",
    "X_test.drop([10392, 16571, 17337], inplace=True)\n",
    "y_test.drop([10392, 16571, 17337], inplace=True)\n",
    "\n",
    "print(\"Training set contains {} recipes in total\".format(len(key_train)))\n",
    "print(\"Test set contains {} recipes in total\".format(len(key_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "bow_transformer = CountVectorizer(analyzer=text_process, min_df = 10).fit(X_train['ingredient']) # Bag of Words\n",
    "print(len(bow_transformer.vocabulary_)) # Print total number of vocab words\n",
    "print(bow_transformer.get_feature_names()) # Print all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform data to bag of words\n",
    "ingredient_bow_train = bow_transformer.transform(X_train['ingredient']) # Transform train dataset to Bag of Words\n",
    "ingredient_bow_test = bow_transformer.transform(X_test['ingredient']) # Transform test dataset to Bag of Words\n",
    "print('Shape of Sparse Matrix: ', ingredient_bow_train.shape) # matrix size (number of recipes, total number of words)\n",
    "print('Amount of Non-Zero occurences: ', ingredient_bow_train.nnz) \n",
    "sparsity = (100.0 * ingredient_bow_train.nnz / (ingredient_bow_train.shape[0] * ingredient_bow_train.shape[1]))\n",
    "print('sparsity: {}'.format(sparsity)) # matrix sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle files\n",
    "import pickle\n",
    "bow_transformer = pickle.load(open('bow_transformer_2.sav','rb'))\n",
    "print(len(bow_transformer.vocabulary_)) # Print total number of vocab words\n",
    "print(bow_transformer.get_feature_names()) # Print all words\n",
    "ingredient_bow_train = pickle.load(open('ingredient_bow_train_2.sav','rb'))\n",
    "ingredient_bow_test = pickle.load(open('ingredient_bow_test_2.sav','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, bow_transformer, performCV=True, useTrainCV=False, printFeatureImportance=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \"\"\"\n",
    "    Perform cross-validation on training data. Returns model performance and feature importance. \n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain, predictors)\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain)\n",
    "    \n",
    "    #Perform cross-validation: performCV = True for Random Forest and Gradient Boosting. False for XGBoost.\n",
    "    if performCV == True:\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        cv_score = cross_val_score(alg, dtrain, predictors, cv=cv_folds, scoring='neg_root_mean_squared_error')\n",
    "    else:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain, label=predictors)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    from scipy import stats\n",
    "    from sklearn import metrics\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(dtrain_predictions,predictors)\n",
    "    print('Slope: ', round(slope,2))\n",
    "    print('Intercept: ', round(intercept))\n",
    "    print('Coefficient of Determinant: ', round(r_value**2,2))\n",
    "    print('p-value: ', p_value)\n",
    "    print('Standard Error: ', round(std_err,2)) # standard error of the slope\n",
    "    print('RMSE:', round(np.sqrt(metrics.mean_squared_error(dtrain_predictions, predictors)),2))\n",
    "\n",
    "    if performCV:\n",
    "        print (\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "        \n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        feat_imp = pd.DataFrame(alg.feature_importances_,bow_transformer.get_feature_names(), columns=['coeff'])\n",
    "        print(feat_imp.sort_values(by='coeff', ascending=False).head())\n",
    "        import matplotlib.pyplot as plt\n",
    "        feat_imp['coeff'].sort_values(ascending=False).head().plot(kind='barh', title='Feature Importances').invert_yaxis()\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "\n",
    "    \n",
    "def model_test(alg, bow_train, y_train, bow_test, y_test, linearmodel = False, plot = True, features = True, resid = False, QQ = False):\n",
    "    \"\"\"\n",
    "    Fit model and validate using test data. Returns model performance, feature importance, residual plot, and QQ plot.\n",
    "    \n",
    "    If using linear regression, specify linearmodel=True\n",
    "    \"\"\"\n",
    "    alg.fit(bow_train, y_train['totalCal'])\n",
    "    predictions = pd.DataFrame(y_test['totalCal'])\n",
    "    predictions['totalCal'] = alg.predict(bow_test)\n",
    "    predictions['calPerServing'] = predictions['totalCal']/y_test['servings']\n",
    "\n",
    "    # Model Performance\n",
    "    from scipy import stats\n",
    "    from sklearn import metrics\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(predictions['calPerServing'],y_test['calPerServing'])\n",
    "    print('Slope: ', round(slope,2))\n",
    "    print('Intercept: ', round(intercept))\n",
    "    print('Coefficient of Determinant: ', round(r_value**2,2))\n",
    "    print('p-value: ', p_value)\n",
    "    print('Standard Error: ', round(std_err,2)) # standard error of the slope\n",
    "    print('RMSE:', round(np.sqrt(metrics.mean_squared_error(y_test['calPerServing'], predictions['calPerServing'])),2))\n",
    "    \n",
    "    # Feature Importance\n",
    "    if linearmodel == True:\n",
    "        feat_imp = pd.DataFrame(linreg.coef_,bow_transformer.get_feature_names(), columns=['coeff'])\n",
    "        print(feat_imp.sort_values(by='coeff', ascending=False).head())\n",
    "    else:\n",
    "        feat_imp = pd.DataFrame(alg.feature_importances_, bow_transformer.get_feature_names(), columns=['coeff'])\n",
    "        print('Number of features removed: ', len(feat_imp[feat_imp['coeff']==0])) # number of features removed\n",
    "        print(feat_imp.sort_values(by='coeff', ascending=False).head())\n",
    "\n",
    "    # Model visualization\n",
    "    if plot == True:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "        sns.set_context('poster', font_scale=1)\n",
    "        ax1 = sns.regplot(y_test['calPerServing'],predictions['calPerServing'])\n",
    "        ax1.set_xlabel('True Calories per Serving')\n",
    "        ax1.set_ylabel('Predicted Calories per Serving')\n",
    "\n",
    "    if features == True:\n",
    "        fig = plt.figure()\n",
    "        sns.set_context('poster', font_scale=1)\n",
    "        ax2 = feat_imp.sort_values(by='coeff', ascending=False).head().plot(kind='barh', title='Feature Importances',legend=False).invert_yaxis()\n",
    "\n",
    "    if resid == True:\n",
    "        sns.set_context('notebook', font_scale=1)\n",
    "        fig = plt.figure()\n",
    "        ax3 = sns.distplot(y_test['calPerServing']-predictions['calPerServing'])\n",
    "        ax3.set_xlabel('Residual Calories per Serving', fontsize=20)\n",
    "        ax3.set_ylabel('Distribution', fontsize=20)\n",
    "    \n",
    "    if QQ == True:\n",
    "        import statsmodels.api as sm\n",
    "        sns.set_context('notebook', font_scale=1)\n",
    "        fig = plt.figure()\n",
    "        ax4 = sm.qqplot(predictions['totalCal'], line='s')\n",
    "    \n",
    "    sns.set_context('notebook', font_scale=1)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "model_test(linreg, ingredient_bow_train, y_train, ingredient_bow_test, y_test, linearmodel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "mid_model = RandomForestRegressor()\n",
    "modelfit(mid_model, ingredient_bow_train, y_train['totalCal'],bow_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_test1 = {'n_estimators':range(10,241,5)}\n",
    "gsearch1 = GridSearchCV(estimator = RandomForestRegressor(max_features='sqrt', random_state=10), \n",
    "param_grid = param_test1, scoring='neg_root_mean_squared_error', n_jobs=4,cv=5)\n",
    "gsearch1.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch1.best_params_, gsearch1.best_score_ # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {'max_depth':range(45,55,1)}\n",
    "gsearch2 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=195, max_features='sqrt', random_state=10), \n",
    "param_grid = param_test2, scoring='neg_root_mean_squared_error', n_jobs=4,cv=5)\n",
    "gsearch2.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch2.best_params_, gsearch2.best_score_ # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {'min_samples_split':range(1,20,1)}\n",
    "gsearch3 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=195, max_depth=51, max_features='sqrt', random_state=10), \n",
    "param_grid = param_test3, scoring='neg_root_mean_squared_error', n_jobs=4,cv=5)\n",
    "gsearch3.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch3.best_params_, gsearch3.best_score_ # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {'min_samples_leaf':range(1,15,1)}\n",
    "gsearch4 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=195, max_depth=51, min_samples_split=2, max_features='sqrt', random_state=10), \n",
    "param_grid = param_test4, scoring='neg_root_mean_squared_error', n_jobs=4,cv=5)\n",
    "gsearch4.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch4.best_params_, gsearch4.best_score_ # Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized grid search\n",
    "This takes ~ 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_param = {'n_estimators': [50, 100, 200, 400, 800] , #number of trees\n",
    "             'max_features': ['auto','sqrt','log2'], # max number of features to consider at every split\n",
    "             'max_depth': [10, 20, 30, 40, 50], # max levels in a tree\n",
    "             'min_samples_split': [2,5,10,15,20], # min number of samples required to split a node\n",
    "             'min_samples_leaf': [1,2,5,10,15] # min number of samples required at each leaf node\n",
    "             }\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "RFR = RandomForestRegressor(random_state=10)\n",
    "RFR_random = RandomizedSearchCV(estimator=RFR, param_distributions = grid_param, n_iter=500, cv=5, \n",
    "                                verbose=2, random_state=42, n_jobs=-1)\n",
    "RFR_random.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "RFR_random.best_params_, RFR_random.best_score_ # Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF_model = RandomForestRegressor(n_estimators=195, min_samples_split=2, min_samples_leaf=1, \n",
    "                                  max_features='sqrt', max_depth=51, random_state=10)\n",
    "\n",
    "model_test(RF_model, ingredient_bow_train, y_train, ingredient_bow_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBR baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "mid_model = GradientBoostingRegressor(loss=\"ls\")\n",
    "modelfit(mid_model, ingredient_bow_train, y_train['totalCal'],bow_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix learning rate & Number of estimators for tuning tree-based parameters\n",
    "`min_samples_split` should be 0.5-1% of the total dataset\n",
    "`min_samples_leaf`should be ~1/10th of `min_samples_split`\n",
    "`learning rate` standard is 0.1. Can go up to 0.3.\n",
    "`n_estimators` should be < 100.\n",
    "\n",
    "If optimal estimators is around 20, lower learning rate to 0.05 and rerun grid search.\n",
    "If optimal estimators is too high (~100), increase learning rate. This will cause tuning of other parameters to take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_test1 = {'n_estimators':range(20,241,10)}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.4, min_samples_split=20, min_samples_leaf=2, max_depth=8, max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test1, scoring='neg_root_mean_squared_error', n_jobs=4,cv=5)\n",
    "gsearch1.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch1.best_params_, gsearch1.best_score_ # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune tree parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,2001,200)}\n",
    "gsearch2 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.4, n_estimators=60, max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test2, scoring='neg_root_mean_squared_error', n_jobs=4, cv=5)\n",
    "gsearch2.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {'min_samples_split':range(900,1200,10), 'min_samples_leaf':range(2,82,4)}\n",
    "gsearch3 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.4, n_estimators=60, max_depth=7, max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test3, scoring='neg_root_mean_squared_error', n_jobs=4, cv=5)\n",
    "gsearch3.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {'max_features':range(7,30,2)}\n",
    "gsearch4 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.4, n_estimators=60, max_depth=7, min_samples_split=930, min_samples_leaf=6, subsample=0.8, random_state=10),\n",
    "param_grid = param_test4, scoring='neg_root_mean_squared_error', n_jobs=4, cv=5)\n",
    "gsearch4.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(gsearch4.best_estimator_, ingredient_bow_train, y_train['totalCal'],bow_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test5 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\n",
    "gsearch5 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.4, n_estimators=60, max_depth=7, min_samples_split=930, min_samples_leaf=6, subsample=0.8, max_features=27, random_state=10),\n",
    "param_grid = param_test5, scoring='neg_root_mean_squared_error', n_jobs=4, cv=5)\n",
    "gsearch5.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune learning rate and number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1/2 learning rate with 2X trees (n_estimators)\n",
    "gbm_tuned_1 = GradientBoostingRegressor(learning_rate=0.2, n_estimators=120, max_depth=7, min_samples_split=930, min_samples_leaf=6, subsample=0.85, max_features=27, random_state=10)\n",
    "modelfit(gbm_tuned_1, ingredient_bow_train, y_train['totalCal'],bow_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/10 learning rate with 10X trees (n_estimators)\n",
    "gbm_tuned_2 = GradientBoostingRegressor(learning_rate=0.04, n_estimators=600, max_depth=7, min_samples_split=930, min_samples_leaf=6, subsample=0.85, max_features=27, random_state=10)\n",
    "modelfit(gbm_tuned_2, ingredient_bow_train, y_train['totalCal'],bow_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1/15 learning rate with 15X trees (n_estimators)\n",
    "gbm_tuned_3 = GradientBoostingRegressor(learning_rate=0.03, n_estimators=900, max_depth=7, min_samples_split=950, min_samples_leaf=6, subsample=0.8, max_features=21, random_state=10)\n",
    "modelfit(gbm_tuned_3, ingredient_bow_train, y_train['totalCal'],bow_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "GBR_model = GradientBoostingRegressor(loss=\"ls\", learning_rate=0.04, n_estimators=600, max_depth=7, min_samples_split=930, min_samples_leaf=6, subsample=0.85, max_features=27, random_state=10)\n",
    "model_test(GBR_model, ingredient_bow_train, y_train, ingredient_bow_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "XGB_model = XGBRegressor()\n",
    "modelfit(XGB_model, ingredient_bow_train, y_train['totalCal'],bow_transformer, performCV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune max depth & min child weight\n",
    "These parameters have the highst impact on model outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':range(5,8,1),\n",
    " 'min_child_weight':range(5,11,1)\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='neg_root_mean_squared_error',n_jobs=4, cv=5)\n",
    "\n",
    "gsearch1.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "gsearch2 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=6,\n",
    " min_child_weight=9, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test2, scoring='neg_root_mean_squared_error',n_jobs=4, cv=5)\n",
    "\n",
    "gsearch2.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch2.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'subsample':[i/100.0 for i in range(60,90,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(70,90,5)]\n",
    "}\n",
    "\n",
    "gsearch3 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=6,\n",
    " min_child_weight=9, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test3, scoring='neg_root_mean_squared_error',n_jobs=4, cv=5)\n",
    "\n",
    "gsearch3.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch3.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune regularization parameters\n",
    "This will reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'reg_alpha':[1e-8, 1e-7, 1e-6, 1e-5, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 100]\n",
    "}\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=6,\n",
    " min_child_weight=9, gamma=0, subsample=0.65, colsample_bytree=0.7,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test4, scoring='neg_root_mean_squared_error',n_jobs=4, cv=5)\n",
    "\n",
    "gsearch4.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch4.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: \n",
    "* max_depth = 6\n",
    "* min_child_weight = 9\n",
    "* gamma = 0\n",
    "* colsample_bytree=0.8\n",
    "* subsample = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = XGBRegressor( learning_rate =0.05, n_estimators=280, max_depth=6,\n",
    " min_child_weight=9, gamma=0, subsample=0.65, colsample_bytree=0.7,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, reg_alpha=1e-6, seed=27)\n",
    "\n",
    "modelfit(model3, ingredient_bow_train, y_train['totalCal'],bow_transformer, performCV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "xgb_model = XGBRegressor(learning_rate =0.05, n_estimators=280, max_depth=6,\n",
    "     min_child_weight=9, gamma=0, subsample=0.65, colsample_bytree=0.7,\n",
    "     objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, reg_alpha=1e-6, seed=27)\n",
    "\n",
    "model_test(xgb_model, ingredient_bow_train, y_train, ingredient_bow_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True # Plot edges on bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = df['calPerServing'].plot(kind='hist',ylim=(0,500),bins=20)\n",
    "ax.set_xlabel('Calories per Serving')\n",
    "ax.set_ylabel('Number of Recipes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(clean_df['totalCal'],kde=False,bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.regplot(x='servings', y='calPerServing', data=clean_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
