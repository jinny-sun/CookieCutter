{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CookieCutter\n",
    "The goal of this project is to predict the calories per serving of a recipe based on the ingredients list.\n",
    "\n",
    "Training data was scraped from AllRecipes.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load bag of words vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "bow_transformer = pickle.load(open('bow_transformer_2.sav','rb'))\n",
    "print(len(bow_transformer.vocabulary_)) # Print total number of vocab words\n",
    "print(bow_transformer.get_feature_names()) # Print all words\n",
    "ingredient_bow_train = pickle.load(open('ingredient_bow_train_2.sav','rb'))\n",
    "ingredient_bow_test = pickle.load(open('ingredient_bow_test_2.sav','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, bow_transformer, performCV=True, useTrainCV=False, printFeatureImportance=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \"\"\"\n",
    "    Perform cross-validation on training data. Returns model performance and feature importance. \n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain, predictors)\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain)\n",
    "    \n",
    "    #Perform cross-validation: performCV = True for Random Forest and Gradient Boosting. False for XGBoost.\n",
    "    if performCV == True:\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        cv_score = cross_val_score(alg, dtrain, predictors, cv=cv_folds, scoring='neg_root_mean_squared_error')\n",
    "    else:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain, label=predictors)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    from scipy import stats\n",
    "    from sklearn import metrics\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(dtrain_predictions,predictors)\n",
    "    print('Slope: ', round(slope,2))\n",
    "    print('Intercept: ', round(intercept))\n",
    "    print('Coefficient of Determinant: ', round(r_value**2,2))\n",
    "    print('p-value: ', p_value)\n",
    "    print('Standard Error: ', round(std_err,2)) # standard error of the slope\n",
    "    print('RMSE:', round(np.sqrt(metrics.mean_squared_error(dtrain_predictions, predictors)),2))\n",
    "\n",
    "    if performCV:\n",
    "        print (\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "        \n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        feat_imp = pd.DataFrame(alg.feature_importances_,bow_transformer.get_feature_names(), columns=['coeff'])\n",
    "        print(feat_imp.sort_values(by='coeff', ascending=False).head())\n",
    "        import matplotlib.pyplot as plt\n",
    "        feat_imp['coeff'].sort_values(ascending=False).head().plot(kind='barh', title='Feature Importances').invert_yaxis()\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "\n",
    "    \n",
    "def model_test(alg, bow_train, y_train, bow_test, y_test, linearmodel = False, plot = True, features = True, resid = False, QQ = False):\n",
    "    \"\"\"\n",
    "    Fit model and validate using test data. Returns model performance, feature importance, residual plot, and QQ plot.\n",
    "    \n",
    "    If using linear regression, specify linearmodel=True\n",
    "    \"\"\"\n",
    "    alg.fit(bow_train, y_train['totalCal'])\n",
    "    predictions = pd.DataFrame(y_test['totalCal'])\n",
    "    predictions['totalCal'] = alg.predict(bow_test)\n",
    "    predictions['calPerServing'] = predictions['totalCal']/y_test['servings']\n",
    "\n",
    "    # Model Performance\n",
    "    from scipy import stats\n",
    "    from sklearn import metrics\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(predictions['calPerServing'],y_test['calPerServing'])\n",
    "    print('Slope: ', round(slope,2))\n",
    "    print('Intercept: ', round(intercept))\n",
    "    print('Coefficient of Determinant: ', round(r_value**2,2))\n",
    "    print('p-value: ', p_value)\n",
    "    print('Standard Error: ', round(std_err,2)) # standard error of the slope\n",
    "    print('RMSE:', round(np.sqrt(metrics.mean_squared_error(y_test['calPerServing'], predictions['calPerServing'])),2))\n",
    "    \n",
    "    # Feature Importance\n",
    "    if linearmodel == True:\n",
    "        feat_imp = pd.DataFrame(linreg.coef_,bow_transformer.get_feature_names(), columns=['coeff'])\n",
    "        print(feat_imp.sort_values(by='coeff', ascending=False).head())\n",
    "    else:\n",
    "        feat_imp = pd.DataFrame(alg.feature_importances_, bow_transformer.get_feature_names(), columns=['coeff'])\n",
    "        print('Number of features removed: ', len(feat_imp[feat_imp['coeff']==0])) # number of features removed\n",
    "        print(feat_imp.sort_values(by='coeff', ascending=False).head())\n",
    "\n",
    "    # Model visualization\n",
    "    if plot == True:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "        sns.set_context('poster', font_scale=1)\n",
    "        ax1 = sns.regplot(y_test['calPerServing'],predictions['calPerServing'])\n",
    "        ax1.set_xlabel('True Calories per Serving')\n",
    "        ax1.set_ylabel('Predicted Calories per Serving')\n",
    "\n",
    "    if features == True:\n",
    "        fig = plt.figure()\n",
    "        sns.set_context('poster', font_scale=1)\n",
    "        ax2 = feat_imp.sort_values(by='coeff', ascending=False).head().plot(kind='barh', title='Feature Importances',legend=False).invert_yaxis()\n",
    "\n",
    "    if resid == True:\n",
    "        sns.set_context('notebook', font_scale=1)\n",
    "        fig = plt.figure()\n",
    "        ax3 = sns.distplot(y_test['calPerServing']-predictions['calPerServing'])\n",
    "        ax3.set_xlabel('Residual Calories per Serving', fontsize=20)\n",
    "        ax3.set_ylabel('Distribution', fontsize=20)\n",
    "    \n",
    "    if QQ == True:\n",
    "        import statsmodels.api as sm\n",
    "        sns.set_context('notebook', font_scale=1)\n",
    "        fig = plt.figure()\n",
    "        ax4 = sm.qqplot(predictions['totalCal'], line='s')\n",
    "    \n",
    "    sns.set_context('notebook', font_scale=1)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load bag of words matrix using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle files\n",
    "import pickle\n",
    "bow_transformer = pickle.load(open('bow_transformer_2.sav','rb'))\n",
    "print(len(bow_transformer.vocabulary_)) # Print total number of vocab words\n",
    "print(bow_transformer.get_feature_names()) # Print all words\n",
    "ingredient_bow_train = pickle.load(open('ingredient_bow_train_2.sav','rb'))\n",
    "ingredient_bow_test = pickle.load(open('ingredient_bow_test_2.sav','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "model_test(linreg, ingredient_bow_train, y_train, ingredient_bow_test, y_test, linearmodel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "mid_model = RandomForestRegressor()\n",
    "modelfit(mid_model, ingredient_bow_train, y_train['totalCal'],bow_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_test1 = {'n_estimators':range(10,241,5)}\n",
    "gsearch1 = GridSearchCV(estimator = RandomForestRegressor(max_features='sqrt', random_state=10), \n",
    "param_grid = param_test1, scoring='neg_root_mean_squared_error', n_jobs=4,cv=5)\n",
    "gsearch1.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch1.best_params_, gsearch1.best_score_ # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {'max_depth':range(45,55,1)}\n",
    "gsearch2 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=195, max_features='sqrt', random_state=10), \n",
    "param_grid = param_test2, scoring='neg_root_mean_squared_error', n_jobs=4,cv=5)\n",
    "gsearch2.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch2.best_params_, gsearch2.best_score_ # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {'min_samples_split':range(1,20,1)}\n",
    "gsearch3 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=195, max_depth=51, max_features='sqrt', random_state=10), \n",
    "param_grid = param_test3, scoring='neg_root_mean_squared_error', n_jobs=4,cv=5)\n",
    "gsearch3.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch3.best_params_, gsearch3.best_score_ # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {'min_samples_leaf':range(1,15,1)}\n",
    "gsearch4 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=195, max_depth=51, min_samples_split=2, max_features='sqrt', random_state=10), \n",
    "param_grid = param_test4, scoring='neg_root_mean_squared_error', n_jobs=4,cv=5)\n",
    "gsearch4.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch4.best_params_, gsearch4.best_score_ # Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized grid search\n",
    "This takes ~ 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_param = {'n_estimators': [50, 100, 200, 400, 800] , #number of trees\n",
    "             'max_features': ['auto','sqrt','log2'], # max number of features to consider at every split\n",
    "             'max_depth': [10, 20, 30, 40, 50], # max levels in a tree\n",
    "             'min_samples_split': [2,5,10,15,20], # min number of samples required to split a node\n",
    "             'min_samples_leaf': [1,2,5,10,15] # min number of samples required at each leaf node\n",
    "             }\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "RFR = RandomForestRegressor(random_state=10)\n",
    "RFR_random = RandomizedSearchCV(estimator=RFR, param_distributions = grid_param, n_iter=500, cv=5, \n",
    "                                verbose=2, random_state=42, n_jobs=-1)\n",
    "RFR_random.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "RFR_random.best_params_, RFR_random.best_score_ # Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF_model = RandomForestRegressor(n_estimators=195, min_samples_split=2, min_samples_leaf=1, \n",
    "                                  max_features='sqrt', max_depth=51, random_state=10)\n",
    "\n",
    "model_test(RF_model, ingredient_bow_train, y_train, ingredient_bow_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBR baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "mid_model = GradientBoostingRegressor(loss=\"ls\")\n",
    "modelfit(mid_model, ingredient_bow_train, y_train['totalCal'],bow_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix learning rate & Number of estimators for tuning tree-based parameters\n",
    "`min_samples_split` should be 0.5-1% of the total dataset\n",
    "`min_samples_leaf`should be ~1/10th of `min_samples_split`\n",
    "`learning rate` standard is 0.1. Can go up to 0.3.\n",
    "`n_estimators` should be < 100.\n",
    "\n",
    "If optimal estimators is around 20, lower learning rate to 0.05 and rerun grid search.\n",
    "If optimal estimators is too high (~100), increase learning rate. This will cause tuning of other parameters to take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_test1 = {'n_estimators':range(20,241,10)}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.4, min_samples_split=20, min_samples_leaf=2, max_depth=8, max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test1, scoring='neg_root_mean_squared_error', n_jobs=4,cv=5)\n",
    "gsearch1.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch1.best_params_, gsearch1.best_score_ # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune tree parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,2001,200)}\n",
    "gsearch2 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.4, n_estimators=60, max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test2, scoring='neg_root_mean_squared_error', n_jobs=4, cv=5)\n",
    "gsearch2.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {'min_samples_split':range(900,1200,10), 'min_samples_leaf':range(2,82,4)}\n",
    "gsearch3 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.4, n_estimators=60, max_depth=7, max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test3, scoring='neg_root_mean_squared_error', n_jobs=4, cv=5)\n",
    "gsearch3.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {'max_features':range(7,30,2)}\n",
    "gsearch4 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.4, n_estimators=60, max_depth=7, min_samples_split=930, min_samples_leaf=6, subsample=0.8, random_state=10),\n",
    "param_grid = param_test4, scoring='neg_root_mean_squared_error', n_jobs=4, cv=5)\n",
    "gsearch4.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(gsearch4.best_estimator_, ingredient_bow_train, y_train['totalCal'],bow_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test5 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\n",
    "gsearch5 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.4, n_estimators=60, max_depth=7, min_samples_split=930, min_samples_leaf=6, subsample=0.8, max_features=27, random_state=10),\n",
    "param_grid = param_test5, scoring='neg_root_mean_squared_error', n_jobs=4, cv=5)\n",
    "gsearch5.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune learning rate and number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1/2 learning rate with 2X trees (n_estimators)\n",
    "gbm_tuned_1 = GradientBoostingRegressor(learning_rate=0.2, n_estimators=120, max_depth=7, min_samples_split=930, min_samples_leaf=6, subsample=0.85, max_features=27, random_state=10)\n",
    "modelfit(gbm_tuned_1, ingredient_bow_train, y_train['totalCal'],bow_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/10 learning rate with 10X trees (n_estimators)\n",
    "gbm_tuned_2 = GradientBoostingRegressor(learning_rate=0.04, n_estimators=600, max_depth=7, min_samples_split=930, min_samples_leaf=6, subsample=0.85, max_features=27, random_state=10)\n",
    "modelfit(gbm_tuned_2, ingredient_bow_train, y_train['totalCal'],bow_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1/15 learning rate with 15X trees (n_estimators)\n",
    "gbm_tuned_3 = GradientBoostingRegressor(learning_rate=0.03, n_estimators=900, max_depth=7, min_samples_split=950, min_samples_leaf=6, subsample=0.8, max_features=21, random_state=10)\n",
    "modelfit(gbm_tuned_3, ingredient_bow_train, y_train['totalCal'],bow_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "GBR_model = GradientBoostingRegressor(loss=\"ls\", learning_rate=0.04, n_estimators=600, max_depth=7, min_samples_split=930, min_samples_leaf=6, subsample=0.85, max_features=27, random_state=10)\n",
    "model_test(GBR_model, ingredient_bow_train, y_train, ingredient_bow_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "XGB_model = XGBRegressor()\n",
    "modelfit(XGB_model, ingredient_bow_train, y_train['totalCal'],bow_transformer, performCV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune max depth & min child weight\n",
    "These parameters have the highst impact on model outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':range(5,8,1),\n",
    " 'min_child_weight':range(5,11,1)\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='neg_root_mean_squared_error',n_jobs=4, cv=5)\n",
    "\n",
    "gsearch1.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "gsearch2 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=6,\n",
    " min_child_weight=9, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test2, scoring='neg_root_mean_squared_error',n_jobs=4, cv=5)\n",
    "\n",
    "gsearch2.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch2.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'subsample':[i/100.0 for i in range(60,90,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(70,90,5)]\n",
    "}\n",
    "\n",
    "gsearch3 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=6,\n",
    " min_child_weight=9, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test3, scoring='neg_root_mean_squared_error',n_jobs=4, cv=5)\n",
    "\n",
    "gsearch3.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch3.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune regularization parameters\n",
    "This will reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'reg_alpha':[1e-8, 1e-7, 1e-6, 1e-5, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 100]\n",
    "}\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=6,\n",
    " min_child_weight=9, gamma=0, subsample=0.65, colsample_bytree=0.7,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test4, scoring='neg_root_mean_squared_error',n_jobs=4, cv=5)\n",
    "\n",
    "gsearch4.fit(ingredient_bow_train, y_train['totalCal'])\n",
    "gsearch4.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: \n",
    "* max_depth = 6\n",
    "* min_child_weight = 9\n",
    "* gamma = 0\n",
    "* colsample_bytree=0.8\n",
    "* subsample = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = XGBRegressor( learning_rate =0.05, n_estimators=280, max_depth=6,\n",
    " min_child_weight=9, gamma=0, subsample=0.65, colsample_bytree=0.7,\n",
    " objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, reg_alpha=1e-6, seed=27)\n",
    "\n",
    "modelfit(model3, ingredient_bow_train, y_train['totalCal'],bow_transformer, performCV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "xgb_model = XGBRegressor(learning_rate =0.05, n_estimators=280, max_depth=6,\n",
    "     min_child_weight=9, gamma=0, subsample=0.65, colsample_bytree=0.7,\n",
    "     objective= 'reg:squarederror', nthread=4, scale_pos_weight=1, reg_alpha=1e-6, seed=27)\n",
    "\n",
    "model_test(xgb_model, ingredient_bow_train, y_train, ingredient_bow_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
