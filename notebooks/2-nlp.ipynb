{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CookieCutter\n",
    "The goal of this project is to predict the calories per serving of a recipe based on the ingredients list.\n",
    "\n",
    "Training data was scraped from AllRecipes.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(tokens, pos_tag):\n",
    "    \"\"\"\n",
    "    Select tokens that have noun part of speech tag\n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    return ([token[0] for token in tagged if token[1] in pos_tag])\n",
    "\n",
    "\n",
    "def word_quantity(ing_column, norm_quant_column, orig_dataframe):\n",
    "    \"\"\"\n",
    "    Repeat word based on quantity of ingredient.\n",
    "    \"\"\"\n",
    "    dummy_df = orig_dataframe.copy()\n",
    "    dummy_df['ingredient'] = dummy_df[ing_column].astype(str) + ' '\n",
    "    zipped = list(zip(dummy_df[ing_column], dummy_df[norm_quant_column]))\n",
    "    inglist = [t[0] * t[1] for t in zipped]\n",
    "    final_df = pd.DataFrame(inglist, columns=['ingredient'])\n",
    "    final_df[[\n",
    "        'recipe_key', 'totalCal', 'calPerServing', 'name', 'ingredient_key'\n",
    "    ]] = orig_dataframe[[\n",
    "        'recipe_key', 'totalCal', 'calPerServing', 'name', 'index'\n",
    "    ]]\n",
    "\n",
    "    # Create multiIndex / hierarchical Dataframe\n",
    "    tuples = list(zip(*[final_df['recipe_key'], final_df['ingredient_key']]))\n",
    "    index = pd.MultiIndex.from_tuples(tuples,\n",
    "                                      names=['recipe_key', 'ingredient_key'])\n",
    "    final_df.set_index(index, inplace=True)\n",
    "    final_df.rename(columns={'recipe_key': 'key'}, inplace=True)\n",
    "    #     return(final_df)\n",
    "\n",
    "    X_ing = final_df.groupby('recipe_key')['ingredient'].apply(\n",
    "        ' '.join)  # join list into one string per recipe\n",
    "    X_ing = pd.DataFrame(X_ing)\n",
    "    return (X_ing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv('clean_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize ingredient text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert measurements to normalized unit  (1 Unit= 1 grams)\n",
    "clean_df['unit'] = np.where(clean_df.ingredient.str.contains(\"dash\"), .3,\n",
    "            np.where(clean_df.ingredient.str.contains(\"pinch\"), .6,\n",
    "            np.where(clean_df.ingredient.str.contains(\"teaspoon\"), 5, \n",
    "            np.where(clean_df.ingredient.str.contains(\"tablespoon\"), 3,\n",
    "            np.where(clean_df.ingredient.str.contains(\"fluid\"), 30,\n",
    "            np.where(clean_df.ingredient.str.contains(\"cup\"), 240, \n",
    "            np.where(clean_df.ingredient.str.contains(\"pint\"), 473,\n",
    "            np.where(clean_df.ingredient.str.contains(\"quart\"), 980,\n",
    "            np.where(clean_df.ingredient.str.contains(\"ounce\"), 28,\n",
    "            np.where(clean_df.ingredient.str.contains(\"oz\"), 28, \n",
    "            np.where(clean_df.ingredient.str.contains(\"pound\"), 454,\n",
    "            np.where(clean_df.ingredient.str.contains(\"rack\"), 908,\n",
    "            np.where(clean_df.ingredient.str.contains(\"small\"), 50,\n",
    "            np.where(clean_df.ingredient.str.contains(\"medium\"), 60,\n",
    "            np.where(clean_df.ingredient.str.contains(\"large\"), 70,\n",
    "            3))))))))))))))) \n",
    "\n",
    "# Tokenization\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "clean_df['ingredient']=[text_process(x) for x in clean_df['ingredient']]\n",
    "\n",
    "# Total quantity of each ingredient needed for recipe (grams* quantity) and condense into a list.\n",
    "clean_df['norm_quant'] = round(clean_df['unit']*clean_df['quantity'])\n",
    "clean_df['norm_quant'] = clean_df['norm_quant'].astype(int)\n",
    "\n",
    "# One word per ingredient - keep only nouns, join multiple words as one string\n",
    "clean_df['ingredient'] = [pos_tagger(tokens, ['NN']) for tokens in clean_df['ingredient']]\n",
    "clean_df['ingredient'] = [''.join(tokens) for tokens in clean_df['ingredient']]\n",
    "\n",
    "# Repeat word by normalized quantity\n",
    "X_ing = word_quantity('ingredient','norm_quant',clean_df)\n",
    "X_ing[['orig_ing', 'name', 'servings']] = df.set_index('recipe_key')[['ingredients', 'name', 'servings']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create feature and outcome dataframe\n",
    "y_cal = df.set_index('recipe_key')[['totalCal', 'calPerServing', 'name','servings']].sort_index().copy()\n",
    "X_keys = df.reset_index(drop=True)['recipe_key']\n",
    "\n",
    "# Train test split (80:20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "key_train, key_test, y_train, y_test = train_test_split(\n",
    "    X_keys, y_cal, test_size=0.2, random_state=101)\n",
    "\n",
    "# Separate feature and outcome dataframes based on key\n",
    "X_train = X_ing.loc[key_train]\n",
    "X_test = X_ing.loc[key_test]\n",
    "y_train = y_cal.loc[key_train]\n",
    "y_test = y_cal.loc[key_test]\n",
    "\n",
    "X_train.sort_index(inplace=True)\n",
    "X_test.sort_index(inplace=True)\n",
    "y_train.sort_index(inplace=True)\n",
    "y_test.sort_index(inplace=True)\n",
    "\n",
    "# Remove extreme edge cases\n",
    "X_test.drop([10392, 16571, 17337], inplace=True)\n",
    "y_test.drop([10392, 16571, 17337], inplace=True)\n",
    "\n",
    "print(\"Training set contains {} recipes in total\".format(len(key_train)))\n",
    "print(\"Test set contains {} recipes in total\".format(len(key_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "bow_transformer = CountVectorizer(analyzer=text_process, min_df = 10).fit(X_train['ingredient']) # Bag of Words\n",
    "print(len(bow_transformer.vocabulary_)) # Print total number of vocab words\n",
    "print(bow_transformer.get_feature_names()) # Print all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform data to bag of words\n",
    "ingredient_bow_train = bow_transformer.transform(X_train['ingredient']) # Transform train dataset to Bag of Words\n",
    "ingredient_bow_test = bow_transformer.transform(X_test['ingredient']) # Transform test dataset to Bag of Words\n",
    "print('Shape of Sparse Matrix: ', ingredient_bow_train.shape) # matrix size (number of recipes, total number of words)\n",
    "print('Amount of Non-Zero occurences: ', ingredient_bow_train.nnz) \n",
    "sparsity = (100.0 * ingredient_bow_train.nnz / (ingredient_bow_train.shape[0] * ingredient_bow_train.shape[1]))\n",
    "print('sparsity: {}'.format(sparsity)) # matrix sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save bag of words vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(bow_transformer,open('bow_transformer_3.sav','wb'))\n",
    "pickle.dump(ingredient_bow_train,open('ingredient_bow_train_3.sav','wb'))\n",
    "pickle.dump(ingredient_bow_test,open('ingredient_bow_test_3.sav','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
